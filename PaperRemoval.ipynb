{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Merge\n",
    "### Merge scopus and web of science (WoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('output_audit/scopus.csv', header=None)\n",
    "t2 = pd.read_csv('output_audit/wos.csv', header=None)\n",
    "\n",
    "scopus = t1.values.tolist()\n",
    "wos = t2.values.tolist()\n",
    "\n",
    "with open('output_audit/merge.csv', 'w', newline='', encoding='utf-8-sig') as outFile, open('output_old/remove.csv', 'w',\n",
    "                                                                                newline='',\n",
    "                                                                                encoding='utf-8-sig') as removeFile:\n",
    "\n",
    "    line = ['Authors', 'Author Full Names', 'Title', 'Source Title', 'Document Type', 'Publisher',\n",
    "            'Publication Year', 'Volume', 'Issue', 'DOI', 'Link', 'ISSN', 'ISBN', 'Number of Pages',\n",
    "            'Author Keywords', 'Language', 'Source', 'Abstract']\n",
    "    csv_writer = csv.writer(outFile)\n",
    "    csv_writer.writerow(line)\n",
    "\n",
    "    nanKey_num = 0  # number of nan keywords\n",
    "    remove_page = 0  # number of removed papers in page\n",
    "    remove_paper = 0  # number of removed papers in paper\n",
    "\n",
    "    # scopus\n",
    "    for s in scopus[1:]:\n",
    "        page = s[10]\n",
    "        document = s[28]\n",
    "        publisher = s[21]\n",
    "        source_title = s[4]\n",
    "        keywords = s[17]\n",
    "\n",
    "        newline = [s[0], s[15], s[2], source_title, document, publisher, s[3], s[5], s[6], s[12], s[13], s[22], s[23], page,\n",
    "                    keywords, s[31], s[36], s[16]]\n",
    "\n",
    "        # Update the keywords\n",
    "        if str(keywords) == '' or str(keywords) == 'nan':\n",
    "            keywords = s[18]\n",
    "        # Write to Output file\n",
    "        csv_writer = csv.writer(outFile)\n",
    "        csv_writer.writerow(newline)\n",
    "\n",
    "    # web of science\n",
    "    for w in wos[1:]:\n",
    "        page = w[52]\n",
    "        document = w[13]\n",
    "        publisher = w[37]\n",
    "        source_title = w[9]\n",
    "        keywords = w[19]\n",
    "\n",
    "        newline = [w[1], w[5], w[8], source_title, document, publisher, w[46], w[47], w[48], w[56], w[57], w[40], w[42],\n",
    "                    page, keywords, w[12], 'Web of Science', w[21]]\n",
    "\n",
    "        # # Delete pages < 4\n",
    "        # if delete_pages(page):\n",
    "        #     remove_page += 1\n",
    "        #     continue\n",
    "        #\n",
    "        # # Delete abnormal journals\n",
    "        # if str(document) == 'Article':\n",
    "        #     if delete_papers(publisher, source_title):\n",
    "        #         remove_paper += 1\n",
    "        #         # write the removed paper to another file\n",
    "        #         csv_writer = csv.writer(removeFile)\n",
    "        #         csv_writer.writerow(newline)\n",
    "        #         continue\n",
    "\n",
    "        # Update the keywords\n",
    "        if str(keywords) == '' or str(keywords) == 'nan':\n",
    "            keywords = w[20]\n",
    "        # Write to Output file\n",
    "        csv_writer = csv.writer(outFile)\n",
    "        csv_writer.writerow(newline)\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find duplicate files\n",
    "### To keep papers after removing duplicate in scopus and WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.read_csv('output_audit/merge.csv')\n",
    "# find duplicates\n",
    "merge[merge.duplicated(['Title'])].to_csv('output_audit/scopus+wos_duplicate.csv', index=False)\n",
    "# store remaining\n",
    "merge.drop_duplicates(['Title']).to_csv('output_audit/merge_duplicate_rm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unqualified papers\n",
    "### 1. Non-English\n",
    "### 2. Predatory journals or conferences\n",
    "### 3. Small pages (< 5)\n",
    "### 4. Outdated (year < 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pages(page):\n",
    "    if str(page) != '#VALUE!' and str(page).isdigit():\n",
    "        if 4 >= int(page) > 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def delete_papers(publisher, source_title):\n",
    "    for p in predatory:\n",
    "        paper = str(p)\n",
    "        # if str(publisher) in paper or paper in str(publisher) or paper in str(source_title):\n",
    "        #     return True\n",
    "        if paper == publisher or paper == source_title:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "t = pd.read_csv('output_audit/merge_duplicate_nonEnglish_rm.csv', header=None)\n",
    "merge = t.values.tolist()\n",
    "\n",
    "predatory = pd.read_csv('output_audit/Predatory.csv', header=None).values.tolist()\n",
    "\n",
    "with open('output_audit/merge_exclusion_applied.csv', 'w', newline='', encoding='utf-8-sig') as outFile, \\\n",
    "        open('output_audit/remove_predatory.csv', 'w', newline='', encoding='utf-8-sig') as removeFilePred, \\\n",
    "        open('output_audit/remove_page.csv', 'w', newline='', encoding='utf-8-sig') as removeFilePage, \\\n",
    "        open('output_audit/remove_outdated.csv', 'w', newline='', encoding='utf-8-sig') as removeFileOut, \\\n",
    "        open('output_audit/remove_nonEn.csv', 'w', newline='', encoding='utf-8-sig') as removeFileNonEn:\n",
    "    csv_writer = csv.writer(outFile)\n",
    "    csv_writer.writerow(merge[0])\n",
    "\n",
    "    remove_paper = 0  # number of removed papers in paper\n",
    "    remove_page = 0  # number of removed papers in page\n",
    "    remove_outdated = 0 # number of removed papers in outdated paper\n",
    "    remove_nonEnglish = 0\n",
    "    remain_paper = 0\n",
    "\n",
    "    for m in merge[1:]:\n",
    "\n",
    "        # Delete abnormal papers\n",
    "        if delete_papers(m[5], m[3]):\n",
    "            remove_paper += 1\n",
    "            # write the removed paper to another file\n",
    "            csv_writer = csv.writer(removeFilePred)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "\n",
    "        # Delete pages <= 4\n",
    "        if delete_pages(m[13]):\n",
    "            remove_page += 1\n",
    "            # write the removed paper to another file\n",
    "            csv_writer = csv.writer(removeFilePage)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "        if str(m[13]).isdigit():\n",
    "            if int(m[13]) == 1:\n",
    "                m[13] = 'nan'\n",
    "\n",
    "        # Delete the outdated paper\n",
    "        if str(m[6]).isdigit():\n",
    "            if int(m[6]) > 2022 or int(m[6]) < 2015:\n",
    "                remove_outdated += 1\n",
    "                csv_writer = csv.writer(removeFileOut)\n",
    "                csv_writer.writerow(m)\n",
    "                continue\n",
    "        else:\n",
    "            remove_outdated += 1\n",
    "            csv_writer = csv.writer(removeFileOut)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "\n",
    "        # Delete non-English\n",
    "        if str(m[15]) != \"English\":\n",
    "            remove_nonEnglish += 1\n",
    "            csv_writer = csv.writer(removeFileNonEn)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "\n",
    "        # Write to Output file\n",
    "        csv_writer = csv.writer(outFile)\n",
    "        csv_writer.writerow(m)\n",
    "        remain_paper += 1\n",
    "\n",
    "    print(\"The number of removed papers in paper is %d\" % remove_paper)\n",
    "    print(\"The number of removed papers in page is %d\" % remove_page)\n",
    "    print(\"The number of removed papers in outdated is %d\" % remove_outdated)\n",
    "    print(\"The number of removed papers in non-English is %d\" % remove_nonEnglish)\n",
    "    print(\"The number of remaining papers is %d\" % remain_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ScienceDirect Papers\n",
    "### Merge the remaining papers of scopus and WoS with ScienceDirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('output_audit/ScienceDirect(title-abstract).csv', header=None)\n",
    "df2 = pd.read_csv('output_audit/merge_exclusion_applied.csv', header=None)\n",
    "\n",
    "science_direct = df1.values.tolist()\n",
    "origin = df2.values.tolist()\n",
    "\n",
    "with open('output_audit/merge_exclusion+scienceDirect.csv', 'w', newline='', encoding='utf-8-sig') as outFile, \\\n",
    "        open('output_old/remove_sd_duplicate.csv', 'w', newline='', encoding='utf-8-sig') as removeFile:\n",
    "    line = ['Authors', 'Author Full Names', 'Title', 'Source Title', 'Document Type', 'Publisher',\n",
    "            'Publication Year', 'Volume', 'Issue', 'DOI', 'Link', 'ISSN', 'ISBN', 'Number of Pages',\n",
    "            'Author Keywords', 'Language', 'Source', 'Abstract']\n",
    "    csv_writer = csv.writer(outFile)\n",
    "    csv_writer.writerow(line)\n",
    "\n",
    "    for o in origin[1:]:\n",
    "        csv_writer = csv.writer(outFile)\n",
    "        csv_writer.writerow(o)\n",
    "\n",
    "    for sd in science_direct[1:]:\n",
    "        newline = [sd[1], sd[1], sd[3], sd[4], sd[0], sd[4], sd[5], sd[6], sd[7], sd[16], sd[15], sd[14], sd[13],\n",
    "                    sd[8], sd[18], 'English', 'ScienceDirect', sd[17]]\n",
    "        csv_writer = csv.writer(outFile)\n",
    "        csv_writer.writerow(newline)\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find duplicate with ScienceDirect\n",
    "### To keep papers after removing duplicate in scopus, WoS and ScienceDirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_sd = pd.read_csv('output_audit/merge_exclusion+scienceDirect.csv')\n",
    "merge_sd[merge_sd.duplicated(['Title'])].to_csv('output_audit/afterScienceDirect/scienceDirect_duplicate.csv', index=False)\n",
    "merge_sd.drop_duplicates(['Title']).to_csv('output_audit/afterScienceDirect/scienceDirect_duplicate_rm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unqualified papers with ScienceDirect\n",
    "### 1. Non-English\n",
    "### 2. Predatory journals or conferences\n",
    "### 3. Small pages (< 5)\n",
    "### 4. Outdated (year < 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('output_audit/afterScienceDirect/scienceDirect_duplicate_rm.csv', header=None)\n",
    "merge = t.values.tolist()\n",
    "\n",
    "predatory = pd.read_csv('output_audit/Predatory.csv', header=None).values.tolist()\n",
    "\n",
    "with open('output_audit/afterScienceDirect/merge_exclusion_applied.csv', 'w', newline='', encoding='utf-8-sig') as outFile, \\\n",
    "        open('output_audit/afterScienceDirect/remove_predatory.csv', 'w', newline='', encoding='utf-8-sig') as removeFilePred, \\\n",
    "        open('output_audit/afterScienceDirect/remove_page.csv', 'w', newline='', encoding='utf-8-sig') as removeFilePage, \\\n",
    "        open('output_audit/afterScienceDirect/remove_outdated.csv', 'w', newline='', encoding='utf-8-sig') as removeFileOut, \\\n",
    "        open('output_audit/afterScienceDirect/remove_nonEn.csv', 'w', newline='', encoding='utf-8-sig') as removeFileNonEn:\n",
    "    csv_writer = csv.writer(outFile)\n",
    "    csv_writer.writerow(merge[0])\n",
    "\n",
    "    remove_paper = 0  # number of removed papers in paper\n",
    "    remove_page = 0  # number of removed papers in page\n",
    "    remove_outdated = 0 # number of removed papers in outdated paper\n",
    "    remove_nonEnglish = 0\n",
    "    remain_paper = 0\n",
    "\n",
    "    for m in merge[1:]:\n",
    "\n",
    "        # Delete abnormal papers\n",
    "        if delete_papers(m[5], m[3]):\n",
    "            remove_paper += 1\n",
    "            # write the removed paper to another file\n",
    "            csv_writer = csv.writer(removeFilePred)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "\n",
    "        # Delete pages <= 4\n",
    "        if delete_pages(m[13]):\n",
    "            remove_page += 1\n",
    "            # write the removed paper to another file\n",
    "            csv_writer = csv.writer(removeFilePage)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "        if str(m[13]).isdigit():\n",
    "            if int(m[13]) == 1:\n",
    "                m[13] = 'nan'\n",
    "\n",
    "        # Delete the outdated paper\n",
    "        if str(m[6]).isdigit():\n",
    "            if int(m[6]) > 2022 or int(m[6]) < 2015:\n",
    "                remove_outdated += 1\n",
    "                csv_writer = csv.writer(removeFileOut)\n",
    "                csv_writer.writerow(m)\n",
    "                continue\n",
    "        else:\n",
    "            remove_outdated += 1\n",
    "            csv_writer = csv.writer(removeFileOut)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "\n",
    "        # Delete non-English\n",
    "        if str(m[15]) != \"English\":\n",
    "            remove_nonEnglish += 1\n",
    "            csv_writer = csv.writer(removeFileNonEn)\n",
    "            csv_writer.writerow(m)\n",
    "            continue\n",
    "\n",
    "        # Write to Output file\n",
    "        csv_writer = csv.writer(outFile)\n",
    "        csv_writer.writerow(m)\n",
    "        remain_paper += 1\n",
    "\n",
    "    print(\"The number of removed papers in paper is %d\" % remove_paper)\n",
    "    print(\"The number of removed papers in page is %d\" % remove_page)\n",
    "    print(\"The number of removed papers in outdated is %d\" % remove_outdated)\n",
    "    print(\"The number of removed papers in non-English is %d\" % remove_nonEnglish)\n",
    "    print(\"The number of remaining papers is %d\" % remain_paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
